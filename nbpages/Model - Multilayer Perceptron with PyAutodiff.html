<html>
<head>
<style type="text/css">
/**
 * HTML5 âœ° Boilerplate
 *
 * style.css contains a reset, font normalization and some base styles.
 *
 * Credit is left where credit is due.
 * Much inspiration was taken from these projects:
 * - yui.yahooapis.com/2.8.1/build/base/base.css
 * - camendesign.com/design/
 * - praegnanz.de/weblog/htmlcssjs-kickstart
 */


/**
 * html5doctor.com Reset Stylesheet (Eric Meyer's Reset Reloaded + HTML5 baseline)
 * v1.6.1 2010-09-17 | Authors: Eric Meyer & Richard Clark
 * html5doctor.com/html-5-reset-stylesheet/
 */

html, body, div, span, object, iframe,
h1, h2, h3, h4, h5, h6, p, blockquote, pre,
abbr, address, cite, code, del, dfn, em, img, ins, kbd, q, samp,
small, strong, sub, sup, var, b, i, dl, dt, dd, ol, ul, li,
fieldset, form, label, legend,
table, caption, tbody, tfoot, thead, tr, th, td,
article, aside, canvas, details, figcaption, figure,
footer, header, hgroup, menu, nav, section, summary,
time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
  font-size: 100%;
  font: inherit;
  vertical-align: baseline;
}

sup { vertical-align: super; }
sub { vertical-align: sub; }

article, aside, details, figcaption, figure,
footer, header, hgroup, menu, nav, section {
  display: block;
}

blockquote, q { quotes: none; }

blockquote:before, blockquote:after,
q:before, q:after { content: ""; content: none; }

ins { background-color: #ff9; color: #000; text-decoration: none; }

mark { background-color: #ff9; color: #000; font-style: italic; font-weight: bold; }

del { text-decoration: line-through; }

abbr[title], dfn[title] { border-bottom: 1px dotted; cursor: help; }

table { border-collapse: collapse; border-spacing: 0; }

hr { display: block; height: 1px; border: 0; border-top: 1px solid #ccc; margin: 1em 0; padding: 0; }

input, select { vertical-align: middle; }


/**
 * Font normalization inspired by YUI Library's fonts.css: developer.yahoo.com/yui/
 */

body { font:13px/1.231 sans-serif; *font-size:small; } /* Hack retained to preserve specificity */
select, input, textarea, button { font:99% sans-serif; }

/* Normalize monospace sizing:
   en.wikipedia.org/wiki/MediaWiki_talk:Common.css/Archive_11#Teletype_style_fix_for_Chrome */
pre, code, kbd, samp { font-family: monospace, sans-serif; }

em,i { font-style: italic; }
b,strong { font-weight: bold; }

</style>
<style type="text/css">

/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
 
.hbox {
	display: -webkit-box;
	-webkit-box-orient: horizontal;
	-webkit-box-align: stretch;
 
	display: -moz-box;
	-moz-box-orient: horizontal;
	-moz-box-align: stretch;
 
	display: box;
	box-orient: horizontal;
	box-align: stretch;
}
 
.hbox > * {
	-webkit-box-flex: 0;
	-moz-box-flex: 0;
	box-flex: 0;
}
 
.vbox {
	display: -webkit-box;
	-webkit-box-orient: vertical;
	-webkit-box-align: stretch;
 
	display: -moz-box;
	-moz-box-orient: vertical;
	-moz-box-align: stretch;
 
	display: box;
	box-orient: vertical;
	box-align: stretch;
}
 
.vbox > * {
	-webkit-box-flex: 0;
	-moz-box-flex: 0;
	box-flex: 0;
}
  
.reverse {
	-webkit-box-direction: reverse;
	-moz-box-direction: reverse;
	box-direction: reverse;
}
 
.box-flex0 {
	-webkit-box-flex: 0;
	-moz-box-flex: 0;
	box-flex: 0;
}
 
.box-flex1, .box-flex {
	-webkit-box-flex: 1;
	-moz-box-flex: 1;
	box-flex: 1;
}
 
.box-flex2 {
	-webkit-box-flex: 2;
	-moz-box-flex: 2;
	box-flex: 2;
}
 
.box-group1 {
	-webkit-box-flex-group: 1;
	-moz-box-flex-group: 1;
	box-flex-group: 1;
}
 
.box-group2 {
	-webkit-box-flex-group: 2;
	-moz-box-flex-group: 2;
	box-flex-group: 2;
}
 
.start {
	-webkit-box-pack: start;
	-moz-box-pack: start;
	box-pack: start;
}
 
.end {
	-webkit-box-pack: end;
	-moz-box-pack: end;
	box-pack: end;
}
 
.center {
	-webkit-box-pack: center;
	-moz-box-pack: center;
	box-pack: center;
}

</style>
<style type="text/css">
/**
 * Primary styles
 *
 * Author: IPython Development Team
 */


body {
    overflow: hidden;
}

span#save_widget {
    padding: 5px;
    margin: 0px 0px 0px 300px;
    display:inline-block;
}

span#notebook_name {
    height: 1em;
    line-height: 1em;
    padding: 3px;
    border: none;
    font-size: 146.5%;
}

.ui-menubar-item .ui-button .ui-button-text {
    padding: 0.4em 1.0em;
    font-size: 100%;
}

.ui-menu {
  -moz-box-shadow:    0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow:         0px 6px 10px -1px #adadad;
}

.ui-menu .ui-menu-item a {
    border: 1px solid transparent;
    padding: 2px 1.6em;
}

.ui-menu .ui-menu-item a.ui-state-focus {
    margin: 0;
}

.ui-menu hr {
    margin: 0.3em 0;
}

#menubar_container {
    position: relative;
}

#notification {
    position: absolute;
    right: 3px;
    top: 3px;
    height: 25px;
    padding: 3px 6px;
    z-index: 10;
}

#toolbar {
    padding: 3px 15px;
}

#cell_type {
    font-size: 85%;
}


div#main_app {
    width: 100%;
    position: relative;
}

span#quick_help_area {
    position: static;
    padding: 5px 0px;
    margin: 0px 0px 0px 0px;
}

.help_string {
    float: right;
    width: 170px;
    padding: 0px 5px;
    text-align: left;
    font-size: 85%;
}

.help_string_label {
    float: right;
    font-size: 85%;
}

div#notebook_panel {
    margin: 0px 0px 0px 0px;
    padding: 0px;
}

div#notebook {
    overflow-y: scroll;
    overflow-x: auto;
    width: 100%;
    /* This spaces the cell away from the edge of the notebook area */
    padding: 5px 5px 15px 5px;
    margin: 0px;
    background-color: white;
}

div#pager_splitter {
    height: 8px;
}

div#pager {
    padding: 15px;
    overflow: auto;
    display: none;
}

div.ui-widget-content {
    border: 1px solid #aaa;
    outline: none;
}

.cell {
    border: 1px solid transparent;
}

div.cell {
    width: 100%;
    padding: 5px 5px 5px 0px;
    /* This acts as a spacer between cells, that is outside the border */
    margin: 2px 0px 2px 0px;
}

div.code_cell {
    background-color: white;
}

/* any special styling for code cells that are currently running goes here */
div.code_cell.running {
}

div.prompt {
    /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
    width: 11ex;
    /* This 0.4em is tuned to match the padding on the CodeMirror editor. */
    padding: 0.4em;
    margin: 0px;
    font-family: monospace;
    text-align:right;
}

div.input {
    page-break-inside: avoid;
}

/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
    color: black;
    border: 1px solid #ddd;
    border-radius: 3px;
    background: #f7f7f7;
}

div.input_prompt {
    color: navy;
    border-top: 1px solid transparent;
}

div.output_wrapper {
    /* This is a spacer between the input and output of each cell */
    margin-top: 5px;
    margin-left: 5px;
    /* FF needs explicit width to stretch */
    width: 100%;
    /* this position must be relative to enable descendents to be absolute within it */
    position: relative;
}

/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  
  overflow: auto;
  border-radius: 3px;
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, .8);
}

/* output div while it is collapsed */
div.output_collapsed {
  margin-right: 5px;
}

div.out_prompt_overlay {
  height: 100%;
  padding: 0px;
  position: absolute;
  border-radius: 3px;
}

div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}

div.output_prompt {
    color: darkred;
    /* 5px right shift to account for margin in parent container */
    margin: 0 5px 0 -5px;
}

/* This class is the outer container of all output sections. */
div.output_area {
    padding: 0px;
    page-break-inside: avoid;
}

/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
    padding: 0.4em 0.4em 0.4em 0.4em;
}

/* The rest of the output_* classes are for special styling of the different
   output types */

/* all text output has this class: */
div.output_text {
    text-align: left;
    color: black;
    font-family: monospace;
}

/* stdout/stderr are 'text' as well as 'stream', but pyout/pyerr are *not* streams */
div.output_stream {
    padding-top: 0.0em;
    padding-bottom: 0.0em;
}
div.output_stdout {
}
div.output_stderr {
    background: #fdd; /* very light red background for stderr */
}

div.output_latex {
    text-align: left;
    color: black;
}

div.output_html {
}

div.output_png {
}

div.output_jpeg {
}

div.text_cell {
    background-color: white;
    padding: 5px 5px 5px 5px;
}

div.text_cell_input {
    color: black;
    border: 1px solid #ddd;
    border-radius: 3px;
    background: #f7f7f7;
}

div.text_cell_render {
    font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;
    outline: none;
    resize: none;
    width:  inherit;
    border-style: none;
    padding: 5px;
    color: black;
}

/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font. 
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */

.CodeMirror {
    line-height: 1.231;  /* Changed from 1em to our global default */
}

.CodeMirror-scroll {
    height: auto;     /* Changed to auto to autogrow */
    /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
    /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
    overflow-y: hidden;
    overflow-x: auto; /* Changed from auto to remove scrollbar */
}

/* CSS font colors for translated ANSI colors. */


.ansiblack {color: black;}
.ansired {color: darkred;}
.ansigreen {color: darkgreen;}
.ansiyellow {color: brown;}
.ansiblue {color: darkblue;}
.ansipurple {color: darkviolet;}
.ansicyan {color: steelblue;}
.ansigrey {color: grey;}
.ansibold {font-weight: bold;}

.completions {
    position: absolute;
    z-index: 10;
    overflow: hidden;
    border: 1px solid grey;
}

.completions select {
    background: white;
    outline: none;
    border: none;
    padding: 0px;
    margin: 0px;
    overflow: auto;
    font-family: monospace;
}

option.context {
  background-color: #DEF7FF;
}
option.introspection {
  background-color: #EBF4EB;
}

/*fixed part of the completion*/
.completions p b {
    font-weight:bold;
}

.completions p {
    background: #DDF;
    /*outline: none;
    padding: 0px;*/
    border-bottom: black solid 1px;
    padding: 1px;
    font-family: monospace;
}

pre.dialog {
    background-color: #f7f7f7;
    border: 1px solid #ddd;
    border-radius: 3px;
    padding: 0.4em;
    padding-left: 2em;
}

p.dialog {
    padding : 0.2em;
}

.shortcut_key {
    display: inline-block;
    width: 15ex;
    text-align: right;
    font-family: monospace;
}

.shortcut_descr {
}

/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre, code, kbd, samp { white-space: pre-wrap; }

#fonttest {
    font-family: monospace;
}

</style>
<style type="text/css">
.rendered_html {color: black;}
.rendered_html em {font-style: italic;}
.rendered_html strong {font-weight: bold;}
.rendered_html u {text-decoration: underline;}
.rendered_html :link { text-decoration: underline }
.rendered_html :visited { text-decoration: underline }
.rendered_html h1 {font-size: 197%; margin: .65em 0; font-weight: bold;}
.rendered_html h2 {font-size: 153.9%; margin: .75em 0; font-weight: bold;}
.rendered_html h3 {font-size: 123.1%; margin: .85em 0; font-weight: bold;}
.rendered_html h4 {font-size: 100% margin: 0.95em 0; font-weight: bold;}
.rendered_html h5 {font-size: 85%; margin: 1.5em 0; font-weight: bold;}
.rendered_html h6 {font-size: 77%; margin: 1.65em 0; font-weight: bold;}
.rendered_html ul {list-style:disc; margin: 1em 2em;}
.rendered_html ul ul {list-style:square; margin: 0em 2em;}
.rendered_html ul ul ul {list-style:circle; margin-left: 0em 2em;}
.rendered_html ol {list-style:upper-roman; margin: 1em 2em;}
.rendered_html ol ol {list-style:upper-alpha; margin: 0em 2em;}
.rendered_html ol ol ol {list-style:decimal; margin: 0em 2em;}
.rendered_html ol ol ol ol {list-style:lower-alpha; margin 0em 2em;}
.rendered_html ol ol ol ol ol {list-style:lower-roman; 0em 2em;}

.rendered_html hr {
    color: black;
    background-color: black;
}

.rendered_html pre {
    margin: 1em 2em;
}

.rendered_html blockquote {
    margin: 1em 2em;
}

.rendered_html table {
    border: 1px solid black;
    border-collapse: collapse;
    margin: 1em 2em;
}

.rendered_html td {
    border: 1px solid black;
    text-align: left;
    vertical-align: middle;
    padding: 4px;
}

.rendered_html th {
    border: 1px solid black;
    text-align: left;
    vertical-align: middle;
    padding: 4px;
    font-weight: bold;
}

.rendered_html tr {
    border: 1px solid black;
}    

.rendered_html p + p {
    margin-top: 1em;
}


</style>
<style type="text/css">
/* Overrides of notebook CSS for static HTML export

*/
body {
  overflow: visible;
  padding: 8px;
}
.input_area {
  padding: 0.4em;
}

</style>
<meta charset="UTF-8">
<style type="text/css">
.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
</style>
<script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">

</script>
<script type="text/javascript">
init_mathjax = function() {
    if (window.MathJax) {
        // MathJax loaded
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            },
            displayAlign: 'left', // Change this to 'center' to center equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
        MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
    }
}
init_mathjax();
</script>
</head>
<body>
<div class="text_cell_render border-box-sizing rendered_html">
<h1>Multilayer Perceptron</h1>
<p>In the previous tutorial on linear SVMs, we looked at how training a model
consists in defining a training objective, and then minimizing it with respect
to model parameters by either stochastic or batch gradient descent.</p>
<p>In this tutorial we will augment our SVM with <em>internal layers</em> of <em>hidden
units</em> and turn our linear classifier into a multi-layer architecture.
An MLP with one internal layer is sometimes called a <em>shallow</em> architecture,
in contrast to an MLP with more internal layers which is sometimes called a <em>deep</em> architecture.</p>
<p>An MLP can be viewed as an SVM, where the input $\mathbf{x}$ is first transformed using a
learnt non-linear vector-valued transformation $\Phi$.
The purpose of this transformation is to map the input data into a space where it becomes linearly separable.
The vector $\Phi(\mathbf{x})$ is referred to as a <em>hidden layer</em>.</p>
<p>This tutorial will again tackle the problem of MNIST digit classification.
XXX</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">arange</span><span class="p">,</span> <span class="n">dot</span><span class="p">,</span> <span class="n">maximum</span><span class="p">,</span> <span class="n">ones</span><span class="p">,</span> <span class="n">tanh</span><span class="p">,</span> <span class="n">zeros</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">uniform</span>

<span class="kn">from</span> <span class="nn">skdata</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">import</span> <span class="nn">autodiff</span>

<span class="kn">from</span> <span class="nn">util</span> <span class="kn">import</span> <span class="n">show_filters</span>

<span class="kn">from</span> <span class="nn">util</span> <span class="kn">import</span> <span class="n">hinge</span>
<span class="kn">from</span> <span class="nn">util</span> <span class="kn">import</span> <span class="n">ova_svm_prediction</span>
<span class="kn">from</span> <span class="nn">util</span> <span class="kn">import</span> <span class="n">ova_svm_cost</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="c"># -- load and prepare the data set (even download if necessary)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="s">&#39;float32&#39;</span>
<span class="n">n_examples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>         <span class="c"># -- denoted L in the math expressions</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

<span class="n">data_view</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">views</span><span class="o">.</span><span class="n">OfficialVectorClassification</span><span class="p">(</span><span class="n">x_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data_view</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">x</span><span class="p">[:</span><span class="n">n_examples</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data_view</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">y</span><span class="p">[:</span><span class="n">n_examples</span><span class="p">]</span>
<span class="c"># -- prepare a &quot;1-hot&quot; version of the labels, denoted Y in the math</span>
<span class="n">y1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">n_classes</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">y1</span><span class="p">[</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2>The MLP Model</h2>
<p>Typically the function $\Phi$ is taken to be the function
$\mathbb{R}^{D_0} \rightarrow (-1, 1)^{D_1}$</p>
<p>$$
\Phi(\mathbf{x}; V, \mathbf{c}) = \tanh( \mathbf{x} V + \mathbf{c})
$$</p>
<p>in which $V \in \mathbb{R}^{D_0 \times D_1}$ is called a <em>weight matrix</em>, and
$\mathbf{c} \in \mathbb{R}^{D_1}$ is called a <em>bias vector</em>.
The integer $D_0$ is the number of elements in $\mathbf{x}$ (sometimes, number
of "input units").
The integer $D_1$ is the number of "hidden units" of the MLP.
We abuse notation slightly here by using $\tanh(\mathbf{u})$ for a vector
$\mathbf{u}$ to denote
the vector of values $\tanh(\mathbf{u}<em>i)$.
Sometimes other non-linear scalar functions are used instead of the tanh
function -- whichever one is used is called the _activation function</em> of
layer $\Phi$.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="k">def</span> <span class="nf">tanh_layer</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>When combined with an SVM classifier (recall previous tutorial), the full classification model can be written</p>
<p>$$
    \mathrm{MLP}(\mathbf{x}) = \mathrm{SVM}\left(\Phi(\mathbf{x}; V, \mathbf{c})); W, \mathbf{b} \right)
$$</p>
<p>This sort of MLP (or Artificial Neural Network - ANN) with a single hidden layer
is sometimes represented graphically as follows:</p>
<p><img src="files/images/mlp.png" align=center/></p>
<p>A single hidden layer of this form is sufficient to make the MLP a universal approximator.
However we will see shortly
that there are benefits to using many such hidden layers (deep learning).
See these course notes for an
<a href="http://www.iro.umontreal.ca/~pift6266/H10/notes/mlp.html">introduction to MLPs, the back-propagation algorithm, and how to train MLPs</a>.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="c"># define prediction function</span>
<span class="k">def</span> <span class="nf">mlp_prediction</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">tanh_layer</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ova_svm_prediction</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mlp_cost</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">tanh_layer</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ova_svm_cost</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Training an MLP</h2>
<p>To train an MLP, we learn <strong>all</strong> parameters of the model ($W, \mathbf{b}, V,
\mathbf{c}$) by gradient descent,
just as we learned the parameters $W, \mathbf{b}$ previously when training the
SVM.</p>
<p>The initial values for the weights of a hidden layer ($V$) should be uniformly
sampled from a symmetric interval that depends on the activation function.
For the tanh activation function results obtained in [Xavier10]_ show that the
interval should be
$$
\left[ -\sqrt{\frac{6}{D_0 + D_1}}, \sqrt{\frac{6}{D_0 + D_1}} \right]
$$</p>
<p>For the logistic sigmoid function $1 / (1 + e^{-u})$ the interval is slightly
different:</p>
<p>$$
\left[ -4\sqrt{\frac{6}{D_0 + D_1}},4\sqrt{\frac{6}{D_0 + D_1}} \right]
$$.</p>
<p>This initialization ensures that at least early in training, each neuron operates in a regime of its activation function where information can easily be propagated both upward (activations flowing from inputs to outputs) and backward (gradients flowing from outputs to inputs).</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="c"># initialize input layer parameters</span>
<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">784</span> <span class="c"># -- aka D_0</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">100</span> <span class="c"># -- aka D_1</span>

<span class="n">V</span> <span class="o">=</span> <span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_inputs</span> <span class="o">+</span> <span class="n">n_hidden</span><span class="p">)),</span>
                <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_inputs</span> <span class="o">+</span> <span class="n">n_hidden</span><span class="p">)),</span>
                <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="c"># now allocate the SVM at the top</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>                           
<span class="n">b</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>As before, we train this model using stochastic gradient descent with
mini-batches. The difference is that we modify the cost function to include the
regularization term. <code>L1_reg</code> and <code>L2_reg</code> are the hyperparameters
controlling the weight of these regularization terms in the total cost function.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="c"># SGD minimization</span>

<span class="c"># -- do n_online_loops passes through the data set doing SGD</span>
<span class="c">#    This can be faster at the beginning than L-BFGS</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">online_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_batches</span> <span class="o">=</span> <span class="n">n_examples</span> <span class="o">/</span> <span class="n">online_batch_size</span>
<span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">autodiff</span><span class="o">.</span><span class="n">fmin_sgd</span><span class="p">(</span><span class="n">mlp_cost</span><span class="p">,</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span>
            <span class="n">streams</span><span class="o">=</span><span class="p">{</span>
                <span class="s">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_batches</span><span class="p">,</span> <span class="n">online_batch_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
                <span class="s">&#39;y1&#39;</span><span class="p">:</span> <span class="n">y1</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_batches</span><span class="p">,</span> <span class="n">online_batch_size</span><span class="p">,</span> <span class="n">y1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))},</span>
            <span class="n">loops</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">stepsize</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="n">print_interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;SGD took </span><span class="si">%.2f</span><span class="s"> seconds&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
<span class="n">show_filters</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="c"># L-BFGS minimization</span>

<span class="k">def</span> <span class="nf">batch_criterion</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mlp_cost</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>

<span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">autodiff</span><span class="o">.</span><span class="n">fmin_l_bfgs_b</span><span class="p">(</span><span class="n">batch_criterion</span><span class="p">,</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">maxfun</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">iprint</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">print</span> <span class="s">&#39;final_cost&#39;</span><span class="p">,</span> <span class="n">batch_criterion</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="c"># -- N. B. the output from this command comes from Fortran, so iPython does not see it.</span>
<span class="c">#    To monitor progress, look at the terminal from which you launched ipython</span>
<span class="n">show_filters</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Testing the MLP</h2>
<p>Once the a classifier has been trained, we can test it for generalization accuracy on the test set. We test it by making predictions for the examples in the test set and counting up the number of classification mistakes.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">train_predictions</span> <span class="o">=</span> <span class="n">mlp_prediction</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="n">y</span> <span class="o">!=</span> <span class="n">train_predictions</span>
<span class="k">print</span> <span class="s">&#39;Current train set error rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_errors</span><span class="p">)</span>

<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">mlp_prediction</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">data_view</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">x</span><span class="p">[:])</span>
<span class="n">test_errors</span> <span class="o">=</span> <span class="n">data_view</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">y</span><span class="p">[:]</span> <span class="o">!=</span> <span class="n">test_predictions</span>
<span class="k">print</span> <span class="s">&#39;Current test set error rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_errors</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Exercises</h2>
<p>Try the following exercises out to get a better feel for training MLPs.</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Exercise: Breaking symmetry</h3>
<p>It might seem more natural to initialize all of the MLP parameters to $0$.
What goes wrong when you do this?</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Exercise: SGD vs. L-BFGS</h3>
<p>Try running L-BFGS starting from the original random initialization -- how good does it get on train and test, and how long does it take to find a solution.</p>
<p>Now repeat that with various amounts of SGD before the L-BFGS. What happens?</p>
<p>Now raise the number of examples seen per iteration of SGD. How does this affect the convergence of SGD and its value as a pre-conditioner?</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h3>Exercise: Overfitting</h3>
<p>Regularization</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Tips and Tricks for training MLPs</h2>
<p>There are several hyper-parameters in the above code, which are not (and,
generally speaking, cannot be) optimized by gradient descent.
The design of outer-loop algorithms for optimizing them is a topic of ongoing
research.
Over the last 25 years, researchers have devised various rules of thumb for choosing them.
A very good overview of these tricks can be found in <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient
BackProp</a> by Yann LeCun,
Leon Bottou, Genevieve Orr, and Klaus-Robert Mueller. Here, we summarize
the same issues, with an emphasis on the parameters and techniques that we
actually used in our code.</p>
<h3>Tips and Tricks: Nonlinearity</h3>
<p>Which non-linear activation function should you use in a neural network?
Two of the most common ones are the logistic sigmoid and the tanh functions.
For reasons explained in <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Section 4.4</a>, nonlinearities that
are symmetric around the origin are preferred because they tend to produce
zero-mean inputs to the next layer (which is a desirable property).
Empirically, we have observed that the tanh has better convergence properties.</p>
<h3>Tips and Tricks: Weight initialization</h3>
<p>At initialization we want the weights to be small enough around the origin
so that the activation function operates near its linear regime, where gradients are
the largest. Otherwise, the gradient signal used for learning is attenuated by
each layer as it is propagated from the classifier towards the inputs.
Other desirable properties, especially for deep networks,
are to conserve variance of the activation as well as variance of back-propagated gradients from layer to layer.
This allows information to flow well upward and downward in the network and
reduces discrepancies between layers.
The initialization used above represents a good compromise between these two
constraints.
For mathematical considerations, please refer to [Xavier10]_.</p>
<h3>Tips and Tricks: Learning Rate</h3>
<p>Optimization by stochastic gradient descent is very sensitive to the step size or <em>learning rate</em>.
There is a great deal of literature on how to choose a the learning rate, and how to change it during optimization.
The simplest solution is to use a constant rate. Rule of thumb: try
several log-spaced values ($10^{-1}, 10^{-2}, \ldots$) and narrow the
(logarithmic) grid search to the region where you obtain the lowest
validation error.</p>
<p>Decreasing the learning rate over time can help a model to settle down into a
[local] minimum.
One simple rule for doing that is $\frac{\mu_0}{1 + d\times t}$ where
$\mu_0$ is the initial rate (chosen, perhaps, using the grid search
technique explained above), $d$ is a so-called "decrease constant"
which controls the rate at which the learning rate decreases (typically, a
smaller positive number, $10^{-3}$ and smaller) and $t$ is the epoch/stage.</p>
<p><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Section 4.7</a> details
procedures for choosing a learning rate for each parameter (weight) in our
network and for choosing them adaptively based on the error of the classifier.</p>
<h3>Tips and Tricks: Number of hidden units</h3>
<p>The number of hidden units that gives best results is dataset-dependent.
Generally speaking, the more complicated the input distribution is, the more capacity the network
will require to model it, and so the larger the number of hidden units thatwill be needed (note that the number of weights in a layer, perhaps a more direct
measure of capacity, is $D_0\times D_1$ (recall $D_0$ is the number of inputs and
$D_1$ is the number of hidden units).</p>
<p>Unless we employ some regularization scheme (early stopping or L1/L2
penalties), a typical number of hidden  units vs. generalization performance graph will be U-shaped.</p>
<h3>Tips and Tricks: Norm Regularization</h3>
<p>Typical values to try for the L1/L2 regularization parameter $\lambda$ are $10^{-2}, 10^{-3}, \ldots$.
It can be useful to regularize the topmost layers in an MLP (closest
to and including the classifier itself) to prevent them from overfitting noisy
hidden layer features, and to encourage the features themselves to be more
discriminative.</p>
</div>
</body>
</html>